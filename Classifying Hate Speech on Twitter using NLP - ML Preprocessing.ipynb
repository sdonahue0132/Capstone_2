{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was based on an Analytis Vidhya prompt, which narrows the scope of hate speech to racist and sexist language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # for regular expressions \n",
    "import nltk  # for text manipulation \n",
    "import string \n",
    "import warnings \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv('csv_and_npz_files/train_E6oV3lV.csv') \n",
    "test = pd.read_csv('csv_and_npz_files/test_tweets_anuFYb8.csv')\n",
    "combined = pd.read_csv('csv_and_npz_files/processed_tweets.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I use a Bag of Words approach, and arbitrarily use 1000 features for the normalized training words.\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "bow = bow_vectorizer.fit_transform(combined['tidy_tweet'])\n",
    "\n",
    "# Then Apply a TF-IDF Vectorizer to assign greater importance to those feature words that are unique to each data set. \n",
    "# Words that occur in both data sets receive less weight in subsequent calculations.\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(combined['tidy_tweet']) \n",
    "\n",
    "# We can keep this set of features for future ML application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation with Word2Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6510028, 7536020)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we make an alternative model that is trained using a shallow Neural Network using the Skip-Gram model\n",
    "# This basically studies associations between words and attempts to infer context based on an input word\n",
    "# Weights are applied in the nodes of the network as opposed to individual words.\n",
    "# We are also shrinking the number of words which develop context to just 200.\n",
    "\n",
    "tokenized_tweet = combined['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combined['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#feelingdiet', 0.6099264621734619),\n",
       " ('melani', 0.5826607942581177),\n",
       " ('avocado', 0.5726335048675537),\n",
       " ('#tacotuesday', 0.5583226680755615),\n",
       " ('deadlift', 0.5555545091629028),\n",
       " ('#pushyourself', 0.5494673252105713),\n",
       " ('cookout', 0.5476097464561462),\n",
       " ('#yogu', 0.5425668358802795),\n",
       " ('cuppa', 0.5418993830680847),\n",
       " ('sushi', 0.5393021106719971)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's experiment with our model_w2v\n",
    "# The numbers generated by the model are an estimates which words most strongly correlate with a given context word.\n",
    "\n",
    "model_w2v.wv.most_similar(positive=\"breakfast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deadlift is an interesting entry, but otherwise this seems to have worked effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.5566582083702087),\n",
       " ('hillari', 0.5443348288536072),\n",
       " ('phoni', 0.535092830657959),\n",
       " ('unstabl', 0.5234814882278442),\n",
       " ('unfit', 0.5224140882492065),\n",
       " ('melo', 0.517794132232666),\n",
       " ('nomine', 0.5167006850242615),\n",
       " ('#delegaterevolt', 0.514083743095398),\n",
       " ('potu', 0.5131095051765442),\n",
       " ('unfavor', 0.5121117234230042)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The w2v model we create with Skip Gram takes a given word and outputs a vector of probable associations with the other words in the 200 word bank.  To craft features which will be usable in future ML application, we need to convert each tweet into a vector of possible contexts based on our 200 word dictionary.  This wil result in 200 additional features we can use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary                                     continue\n",
    "            if count != 0:\n",
    "                vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation with Doc2Vec Embedding\n",
    "\n",
    "In this next section, let's use Doc2Vec Embedding to generate additional features.  Doc2Vec can be thought of as an extension of Word2Vec, where the context is guessed for the whole document as opposed to single words within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['when', 'father', 'dysfunct', 'selfish', 'drag', 'kid', 'into', 'dysfunct', '#run'], tags=['tweet_0']),\n",
       " LabeledSentence(words=['thank', '#lyft', 'credit', 'caus', 'they', 'offer', 'wheelchair', 'van', '#disapoint', '#getthank'], tags=['tweet_1']),\n",
       " LabeledSentence(words=['bihday', 'your', 'majesti'], tags=['tweet_2']),\n",
       " LabeledSentence(words=['#model', 'love', 'take', 'with', 'time'], tags=['tweet_3']),\n",
       " LabeledSentence(words=['factsguid', 'societi', '#motiv'], tags=['tweet_4']),\n",
       " LabeledSentence(words=['huge', 'fare', 'talk', 'befor', 'they', 'leav', 'chao', 'disput', 'when', 'they', 'there', '#allshowandnogo'], tags=['tweet_5'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49159/49159 [00:00<00:00, 1833464.85it/s]\n"
     ]
    }
   ],
   "source": [
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model                                   \n",
    "                                  dm_mean=1, # dm = 1 for using mean of the context word vectors                                  \n",
    "                                  vector_size=200, # no. of desired features                                  \n",
    "                                  window=5, # width of the context window                                  \n",
    "                                  negative=7, # if > 0 then negative sampling will be used                                 \n",
    "                                  min_count=5, # Ignores all words with total frequency lower than 2.                                  \n",
    "                                  workers=3, # no. of cores                                  \n",
    "                                  alpha=0.1, # learning rate                                  \n",
    "                                  seed = 23) \n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "model_d2v.train(labeled_tweets, total_examples= len(combined['tidy_tweet']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "for i in range(len(combined)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"csv_and_npz_files/bow.npz\", bow)\n",
    "sparse.save_npz(\"csv_and_npz_files/tfidf.npz\", tfidf)\n",
    "wordvec_df.to_csv('csv_and_npz_files/wordvec_df.csv')\n",
    "docvec_df.to_csv('csv_and_npz_files/docvec_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the Data Processing prior to ML application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
